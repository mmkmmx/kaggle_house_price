---
title: "Predicting house sale prices"
author: "Mike Marett, Belu Chun Ho Chik, Craig Teerlink"
date: "December 13, 2020"
output: 
  pdf_document:
    toc: FALSE
#    number_sections: true
---
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(caret)
library(ggthemes)

d1 <- read_csv("train.csv")
```

##Instructions (from canvas/grading rubric)
Length. No more than 5 pages of text, single spaced, excluding plots and tables. (Submit source code in a separate document.) As noted above, you may write less than 5 pages, provided that you satisfy the requirements for the project. Check the grading rubric on Canvas for specific grading criteria.

Report Format. Your final report should be written and formatted carefully. Consider your audience to be, hypothetically, the manager of a data science team. You should observe the best practices of statistical communication: use graphs when possible, labeling and explaining them, and interpret statistical results using language and quantities that non-statisticians can understand. (Remember: your results could get recycled for non-expert audiences.) Your report should: (1) introduce the problem, (2) describe the data and any cleaning you did, (3) explain your model in detail (how you developed it, and how it differs from and improves upon the model you used for the interim report), and (4) report model performance, including:

RMSE and R2 on the train set.
Estimated RMSE and R2 on the test set.
Your best Kaggle score (log RMSE) and rank.


## Introduction
Introduction. What is the problem you are working on? Briefly but thoroughly describe the Kaggle train/test format, and your ultimate objective in the project.

## Data modeling and cleaning
Data modeling and Cleaning. What sort of data modeling and cleaning decisions did you make? Did you create any variables? Did you refactor any variables? How did you deal with NAs?

(I wrote this in the section below but then realized it makes more sense up here)
Looking at some of the numeric variables a little closer we noticed that some of them were actually categorical in nature.  These fields, MSSubClass & MoSold, were then converted to a factor.  We also created a new column called Age which was the difference between YrSold and YearRemodAdd.  
## Model and model development
Model and Model development. Describe your model. Which variables did you end up using in your model, and why? Which statistical method(s) did you choose and why?

We initially started with a basic linear regression for SalePrice against the cleaned up data for all variables and it had a Kaggle score  of .18. While this was an improvement over the .24 score from our previous five factor model, we started experimenting with various ideas to improve our model.  

The next step involved switching to a linear regression using a log transformed SalePrice as the target variable.  This was done since the data for SalePrice was skewed to the right and included several large outliers:
```{r}
ggplot(d1,aes(SalePrice))+
  geom_histogram() +
  labs(title = "Initial SalePrice Distribution") +
  scale_x_continuous(labels=scales::dollar_format()) +
  theme_void()




```
However the distribution for log(SalePrice) looked a lot better:
```{r}
ggplot(d1,aes(log(SalePrice)))+
  geom_histogram() +
  labs(title = "log(SalePrice) Distribution")+
    theme_void()


```
Using log(SalePrice) as the target variable in a linear regression with all variables further improved the model to a Kaggle score of .15.   

(this could go in the section up above as well) We then created histograms and log transformed variables where they appeared to be skewed to the right with outliers.  We also plotted continuous variables in a scatter plot with SalesPrice to determine if any of those variables could be improved with a log function.  The charts highlighted a few fields that would benefit from a log transformation: LotArea, TotalBsmtSF, GrLivArea.  Making this changes didn't have a significant effect on the RMSE.  

We also noticed that the out of sample RMSE was around .1 lower than the in sample R^2^ indicating that our model was over-fitting.  We then began experimenting with other data models that might offer better results.  Using caret's random forest model ("rf") in conjunction with log(SalePrice) returned favorable RMSE and R^2^ values and a Kaggle Score of .14. 

We then decided to try to work some interaction effects into our model.  First we used dummyVars to transform all the variables into dummy variables. This resulted in the variable count increasing to almost 200 variables.  After that we used dummyVars again to run two-way interactions on all the variables.  This inturn resulted in over 37,000 variables.  We attempted to run this through a simple lm model but continued to get stack overflow errors presumbably due to the sheer volume of variables.  

Realizing that a simple linear regression model is unlikely to be the best model to use with so many variables we looked through Kaggle for other models that might work.  This led us to the LASSO model which could pull in a vast amount of variables and determine which ones were useful and which ones were not.  Ultimately getting the LASSO model to work was just beyond our current understanding.  

We then came across the idea of averaging results from multiple models together.  The thinking was that each model utilizes different methods to drive its predictions and a combination of models could result in a better prediction than the sum of its parts.  This would be similar to how holding uncorrelated assets in an investment portfolio, while individually are risky, can increase returns while lowering risk.  To this end we averaged our random forest model with our linear regression model.  These two models had respective Kaggle scores of .14 and .15, however when we averaged them together the resulting output obtained a Kaggle score of .13.  We experimented with various weightings between the two models and found that a 60/40 blend of rf and lm produced the lowest RMSE.  

## Model performance
Model performance. How did your model perform on the train set? Report train set RMSE and R2. What is your estimate of the modelâ€™s performance on the test set using cross-validation? Report estimated RMSE and R2 on the test set, as well as the score you received back from your Kaggle submission.

## Statistical communication
Statistical communication. Are your figures labelled clearly, accompanied by explanatory captions, and referenced in the text? Likewise, are your tables titled and/or captioned appropriately and referenced in the text? Do the figures and tables chosen improve communication or are they superfluous?

##Overall quality
Overall quality. Are you writing in full sentences? Are there many gram- matical or spelling errors? Are you using the .Rmd or .Rnw formats correctly so that your final document is attractively formatted? Does the report look sharp, or are there issues with the compile from .Rmd such that there is lots of junk code getting printed to the screen?

## Leaderboard
Leaderboard rating. How good is your model compared to those that others reported.





*A Table is input like this*
\begin{tabular}{l|r|r}
Predictor & Odds Ratio & P-value\\
\hline
Intercept & 0.63 & 7.32e-06\\
Store7Yes & 0.38 & 7.24e-07\\
PriceMM & 0.79 & 2.45e-03\\
DiscMM & 1.79 & 3.29e-11\\
LoyalCH & 0.15 & < 2e-16\\
\end{tabular}


